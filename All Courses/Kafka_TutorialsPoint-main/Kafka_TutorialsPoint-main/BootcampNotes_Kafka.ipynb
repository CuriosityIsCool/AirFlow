{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "xTw5dqHvBB_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resources Provided by Ketan\n",
        "\n",
        "* KafKa :- https://www.tutorialspoint.com/apache_kafka/index.htm"
      ],
      "metadata": {
        "id": "HcQ3QNwzBJCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##My References\n",
        "\n",
        "* Apache Kafka Tutorial Series - `Brijesh Gupta` https://youtube.com/playlist?list=PLxv3SnR5bZE82Cv4wozg2uZvaOlDEbO67"
      ],
      "metadata": {
        "id": "VVeksRcc8AbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorials Point / Kafka"
      ],
      "metadata": {
        "id": "6S5HBOAmUSSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apache Kafka - Integration With Storm\n",
        "\n",
        "Storm is very fast and a benchmark clocked it at over a million tuples processed per second per node. Apache Storm runs continuously, consuming data from the configured sources (Spouts) and passes the data down the processing pipeline (Bolts). Com-bined, Spouts and Bolts make a Topology."
      ],
      "metadata": {
        "id": "fVmfSfFXUovi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apache Kafka - Applications\n",
        "\n",
        "Kafka supports many of today's best industrial applications. We will provide a very brief overview of some of the most notable applications of Kafka in this chapter.\n",
        "\n",
        "* `Twitter`\n",
        "Twitter is an online social networking service that provides a platform to send and receive user tweets. Registered users can read and post tweets, but unregistered users can only read tweets. Twitter uses Storm-Kafka as a part of their stream processing infrastructure.\n",
        "\n",
        "* `LinkedIn`\n",
        "Apache Kafka is used at LinkedIn for activity stream data and operational metrics. Kafka mes-saging system helps LinkedIn with various products like LinkedIn Newsfeed, LinkedIn Today for online message consumption and in addition to offline analytics systems like Hadoop. Kafka’s strong durability is also one of the key factors in connection with LinkedIn.\n",
        "\n",
        "* `Netflix`\n",
        "Netflix is an American multinational provider of on-demand Internet streaming media. Netflix uses Kafka for real-time monitoring and event processing.\n",
        "\n",
        "* `Mozilla`\n",
        "Mozilla is a free-software community, created in 1998 by members of Netscape. Kafka will soon be replacing a part of Mozilla current production system to collect performance and usage data from the end-user’s browser for projects like Telemetry, Test Pilot, etc.\n",
        "\n",
        "* `Oracle`\n",
        "Oracle provides native connectivity to Kafka from its Enterprise Service Bus product called OSB (Oracle Service Bus) which allows developers to leverage OSB built-in mediation capabilities to implement staged data pipelines."
      ],
      "metadata": {
        "id": "WKd5mPWHYs5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#YouTube / Apache Kafka Tutorial Series - `Brijesh Gupta`"
      ],
      "metadata": {
        "id": "oSwRWFlv8P5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Kafka ?"
      ],
      "metadata": {
        "id": "grgHoV6J8Zw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Messaging System\n",
        "\n",
        "* Reduces number of connections\n",
        "For example,\n",
        "20 sender services and 30 reciever services will make 20*30 = 600 connections.\n",
        "But if we use a messaging system in between them, connection count will reduce to 20+30 = 50 connections.\n",
        "\n",
        "* record = data = message\n",
        "\n",
        "* Point to point messaging system: `sender`, `receiver`\n",
        "\n",
        "* ----messaging system : `publisher` publishes message in a queue, and then messaging system sends notification to `subscriber` that it can recive message whenever it is free and return the ack that message is recieved so it can be deleted from the queue."
      ],
      "metadata": {
        "id": "xEX_8i9-8itk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kafka is a distributed message streaming platform that uses public subscribe method to stream the records."
      ],
      "metadata": {
        "id": "pOf2l53c_c_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A topic in Kafka is similar to a table in database since both contain data/records."
      ],
      "metadata": {
        "id": "JdpB-o79A653"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brokers manage published messages"
      ],
      "metadata": {
        "id": "AqO4Fi1KCW1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clusters support horizontal scalability meaning multiple brokers(parallel) can be added to the running cluster without disturbing it's running processes."
      ],
      "metadata": {
        "id": "C18tp9EiCvaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying Hands-on using `Python`\n",
        "As i have'nt learned `java` programming"
      ],
      "metadata": {
        "id": "5vkq91xbKpE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Kafka-zookeeper"
      ],
      "metadata": {
        "id": "WnM2ZKHLK_Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kafka-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nDTk3qlLUb9",
        "outputId": "1a8fcff2-7c3c-404e-a766-e33e8d2476be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kazoo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4miMbJUJLtZH",
        "outputId": "5c82c0c2-05e3-4318-bc7e-f72bea54d82c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kazoo\n",
            "  Downloading kazoo-2.9.0-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.6/145.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from kazoo) (1.16.0)\n",
            "Installing collected packages: kazoo\n",
            "Successfully installed kazoo-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up ZooKeeper:\n",
        "(issues with installing and hosting kafka and zookeeper, give more time....)\n",
        "\n",
        "(basically, host zookeeper and kafka on local machine and connect it here..)"
      ],
      "metadata": {
        "id": "ZZdiwAOrLhVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kazoo import client as kz_client\n",
        "\n",
        "zk = kz_client.KazooClient(hosts='localhost:2181')\n",
        "zk.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "SsSg67d7Lj5L",
        "outputId": "29489dca-5d39-490a-e136-c0a2fd3d8b06"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:kazoo.client:Connection dropped: socket connection error: Connection refused\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Cannot assign requested address\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Connection refused\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Cannot assign requested address\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Cannot assign requested address\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Connection refused\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Cannot assign requested address\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Connection refused\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Connection refused\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Cannot assign requested address\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Connection refused\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Cannot assign requested address\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Connection refused\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Cannot assign requested address\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Cannot assign requested address\n",
            "WARNING:kazoo.client:Connection dropped: socket connection error: Connection refused\n",
            "WARNING:kazoo.client:Failed connecting to Zookeeper within the connection retry policy.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KazooTimeoutError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKazooTimeoutError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bb1aa6b7a39a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mzk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkz_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKazooClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'localhost:2181'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mzk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kazoo/client.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Connection time-out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchroot\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKazooTimeoutError\u001b[0m: Connection time-out"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Kafka brokers\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "\n",
        "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
        "consumer = KafkaConsumer('my_topic', bootstrap_servers='localhost:9092')"
      ],
      "metadata": {
        "id": "i8GsOzHyXdyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a topic:\n",
        "producer.send('my_topic', b'Hello, World!')"
      ],
      "metadata": {
        "id": "YJmEKTXQXmDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consume messages:\n",
        "for message in consumer:\n",
        "    print(message)"
      ],
      "metadata": {
        "id": "mL0_plqsX0Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Short Summary\n",
        "Apache Kafka, is an open-source distributed streaming platform that is used to process, store, and transmit large amounts of data in real-time.\n",
        "\n",
        "Here are the basic concepts of Kafka:\n",
        "\n",
        "1. Topic: A topic is a category or feed name to which messages are published by producers.\n",
        "\n",
        "2. Producer: A producer is a process that publishes messages to a Kafka topic.\n",
        "(create a producer application that can publish messages to Kafka topics, including configuring message serialization and setting up callbacks for delivery status.)\n",
        "\n",
        "3. Consumer: A consumer is a process that subscribes to one or more topics and processes the messages published to them.\n",
        "(create a consumer application that can subscribe to Kafka topics, including consuming messages in batches or individually and handling offsets.)\n",
        "\n",
        "4. Broker: A broker is a server that manages the storage and transmission of messages between producers and consumers.\n",
        "\n",
        "5. Partition: A topic can be divided into multiple partitions, which are distributed across multiple brokers. Partitions allow for parallel processing of messages.\n",
        "\n",
        "6. Offset: Each message in a partition is assigned a unique identifier called an offset, which represents its position in the partition.\n",
        "\n",
        "7. Consumer Group: A consumer group is a set of consumers that work together to consume messages from a set of partitions.\n",
        "\n",
        "Some important features of Kafka are:\n",
        "\n",
        "* High Throughput: Kafka is designed to handle high volumes of data and supports millions of messages per second.\n",
        "\n",
        "* Fault Tolerance: Kafka is fault-tolerant and can handle failures in brokers, producers, and consumers.\n",
        "\n",
        "* Scalability: Kafka can scale horizontally by adding more brokers to the cluster.\n",
        "\n",
        "* Durability: Kafka stores messages on disk, which makes them durable and available for consumption even in the event of a failure.\n",
        "\n",
        "* Real-time processing: Kafka allows real-time processing of data and supports both stream processing and batch processing."
      ],
      "metadata": {
        "id": "5AiTEpgBIPdt"
      }
    }
  ]
}